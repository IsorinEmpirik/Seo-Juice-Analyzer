"""
Analyseur de maillage interne - Calcul du "jus SEO"
"""
import logging
from typing import Dict, List, Tuple
from collections import defaultdict, Counter
import numpy as np

logger = logging.getLogger(__name__)


class SEOJuiceAnalyzer:
    """
    Calcule le score de "jus SEO" pour chaque page d'un site web
    en fonction des backlinks externes et du maillage interne
    """

    def __init__(self, config: Dict = None):
        """
        Initialize l'analyseur

        Args:
            config: Configuration de l'algorithme
        """
        self.config = config or {}

        # Param√®tres par d√©faut
        self.backlink_score = self.config.get('backlink_score', 3)
        self.transmission_rate = self.config.get('transmission_rate', 0.85)
        self.content_link_weight = self.config.get('content_link_weight', 9)
        self.navigation_link_weight = self.config.get('navigation_link_weight', 1)
        self.iterations = self.config.get('iterations', 3)
        self.normalize_max = self.config.get('normalize_max', 100)

        # Donn√©es
        self.url_scores = {}  # {url: score}
        self.url_data = {}    # {url: {backlinks, internal_links_received, anchors, etc.}}
        self.internal_links = {}  # {source_url: [liste de liens sortants]}
        self.backlinks = {}   # {url: nombre de backlinks}

    @staticmethod
    def _should_exclude_url(url: str) -> bool:
        """
        D√©termine si une URL doit √™tre exclue de l'analyse

        Args:
            url: L'URL √† v√©rifier

        Returns:
            True si l'URL doit √™tre exclue, False sinon
        """
        # Exclure les URLs en .pdf
        if url.lower().endswith('.pdf'):
            return True

        # Exclure les URLs avec param√®tres GET
        if '?' in url:
            return True

        return False

    def analyze(self, sf_parser, ahrefs_parser, gsc_data: Dict = None) -> Dict:
        """
        Lance l'analyse compl√®te

        Args:
            sf_parser: ScreamingFrogParser avec les liens internes
            ahrefs_parser: AhrefsParser avec les backlinks
            gsc_data: Donn√©es GSC agr√©g√©es par URL (optionnel)

        Returns:
            Dictionnaire avec tous les r√©sultats
        """
        logger.info("=" * 60)
        logger.info("ANALYSE DU MAILLAGE INTERNE - DEBUT")
        logger.info("=" * 60)

        # Stocker les donn√©es GSC pour utilisation dans _calculate_statistics
        self.gsc_data = gsc_data or {}
        if self.gsc_data:
            logger.info(f"Donn√©es GSC disponibles pour {len(self.gsc_data)} URLs")

        # √âtape 1: Initialiser les donn√©es
        self._initialize_data(sf_parser, ahrefs_parser)

        # √âtape 2: Calculer les scores initiaux (backlinks)
        self._calculate_initial_scores()

        # √âtape 3: It√©rations de distribution du jus
        self._run_iterations()

        # √âtape 4: Normaliser les scores
        self._normalize_scores()

        # √âtape 5: Calculer les statistiques
        results = self._calculate_statistics(sf_parser, ahrefs_parser)

        logger.info("=" * 60)
        logger.info("ANALYSE DU MAILLAGE INTERNE - FIN")
        logger.info("=" * 60)

        return results

    def _initialize_data(self, sf_parser, ahrefs_parser):
        """Initialise les structures de donn√©es"""
        logger.info("\n1. INITIALISATION DES DONNEES")
        logger.info("-" * 60)

        # R√©cup√©rer toutes les URLs du site
        all_urls = sf_parser.get_all_urls()
        logger.info(f"URLs uniques trouvees avant filtrage: {len(all_urls)}")

        # D√©terminer le domaine principal (celui qui appara√Æt le plus)
        from urllib.parse import urlparse
        from collections import Counter

        domains = [urlparse(url).netloc for url in all_urls]
        domain_counts = Counter(domains)
        main_domain = domain_counts.most_common(1)[0][0] if domain_counts else None

        logger.info(f"Domaine principal detecte: {main_domain}")

        # Filtrer uniquement les URLs du domaine principal
        all_urls = [url for url in all_urls if urlparse(url).netloc == main_domain]
        logger.info(f"URLs du domaine principal: {len(all_urls)}")

        # Filtrer les URLs √† exclure (.pdf et param√®tres GET)
        excluded_count = sum(1 for url in all_urls if self._should_exclude_url(url))
        all_urls = [url for url in all_urls if not self._should_exclude_url(url)]
        logger.info(f"URLs exclues (.pdf et parametres GET): {excluded_count}")
        logger.info(f"URLs retenues pour l'analyse: {len(all_urls)}")

        # Initialiser les scores √† 0
        for url in all_urls:
            self.url_scores[url] = 0.0
            self.url_data[url] = {
                'backlinks_count': 0,
                'internal_links_received': 0,
                'internal_links_received_content': 0,
                'internal_links_received_navigation': 0,
                'internal_links_sent': 0,
                'anchors': [],
                'status_code': 200,  # Par d√©faut
                'is_error': False
            }

        # Stocker le domaine principal pour filtrage ult√©rieur
        self.main_domain = main_domain

        # R√©cup√©rer les liens internes group√©s par source
        self.internal_links = sf_parser.get_links_by_source()
        logger.info(f"Pages sources (qui font des liens): {len(self.internal_links)}")

        # R√©cup√©rer les backlinks
        raw_backlinks = ahrefs_parser.get_backlink_count_by_url()

        # Filtrer les backlinks vers des URLs exclues
        self.backlinks = {url: count for url, count in raw_backlinks.items()
                         if not self._should_exclude_url(url)}

        excluded_backlinks = len(raw_backlinks) - len(self.backlinks)
        logger.info(f"URLs avec backlinks externes (brut): {len(raw_backlinks)}")
        logger.info(f"Backlinks vers URLs exclues: {excluded_backlinks}")
        logger.info(f"URLs avec backlinks retenus: {len(self.backlinks)}")

        # Mettre √† jour les donn√©es avec les backlinks
        for url, count in self.backlinks.items():
            if url in self.url_data:
                self.url_data[url]['backlinks_count'] = count

        # Compter les liens internes re√ßus et r√©cup√©rer les ancres
        for source_url, links in self.internal_links.items():
            # Filtrer seulement les liens vers le domaine principal ET exclure .pdf et param√®tres GET
            links = [l for l in links
                    if urlparse(l['destination']).netloc == main_domain
                    and not self._should_exclude_url(l['destination'])]

            # Compter les liens sortants
            if source_url in self.url_data:
                self.url_data[source_url]['internal_links_sent'] = len(links)

            for link in links:
                dest_url = link['destination']

                # Compter les liens re√ßus
                if dest_url in self.url_data:
                    self.url_data[dest_url]['internal_links_received'] += 1

                    # Compter s√©par√©ment contenu vs navigation
                    if link['link_position'] in ['Contenu', 'Content']:
                        self.url_data[dest_url]['internal_links_received_content'] += 1
                    else:
                        self.url_data[dest_url]['internal_links_received_navigation'] += 1

                    # Stocker l'ancre si elle n'est pas vide
                    if link['anchor']:
                        self.url_data[dest_url]['anchors'].append(link['anchor'])

                    # Stocker le code de statut
                    if link['status_code']:
                        self.url_data[dest_url]['status_code'] = link['status_code']
                        self.url_data[dest_url]['is_error'] = link['status_code'] != 200

    def _calculate_initial_scores(self):
        """Pr√©pare les scores initiaux (tous √† z√©ro)"""
        logger.info("\n2. PREPARATION DES SCORES INITIAUX")
        logger.info("-" * 60)

        # Les scores sont d√©j√† initialis√©s √† 0 dans _initialize_data
        # Les backlinks seront inject√©s √† CHAQUE it√©ration (pas seulement au d√©but)

        total_backlinks = sum(self.backlinks.values())
        logger.info(f"Total backlinks: {total_backlinks}")
        logger.info(f"Score par backlink (√† chaque it√©ration): {self.backlink_score}")

        # Top 5 des pages avec le plus de backlinks
        sorted_backlinks = sorted(self.backlinks.items(), key=lambda x: x[1], reverse=True)[:5]
        logger.info("\nTop 5 pages avec le plus de backlinks:")
        for url, count in sorted_backlinks:
            logger.info(f"  - {url[:60]}... : {count} backlinks ({count * self.backlink_score:.1f} jus/it√©ration)")

    def _run_iterations(self):
        """Ex√©cute les it√©rations avec conservation du jus et convergence"""
        logger.info(f"\n3. ITERATIONS DE DISTRIBUTION DU JUS (avec convergence)")
        logger.info("-" * 60)

        from urllib.parse import urlparse

        # Calculer le jus inject√© par les backlinks √† chaque it√©ration
        backlinks_juice_per_iteration = sum(count * self.backlink_score for count in self.backlinks.values())
        logger.info(f"Jus inject√© par it√©ration (backlinks): {backlinks_juice_per_iteration:.2f}")
        logger.info(f"Mod√®le: Flux avec perte de 15% √† chaque transmission")
        logger.info(f"Poids des liens: Contenu = {self.content_link_weight}, Navigation = {self.navigation_link_weight}")

        # Identifier la homepage (page avec le plus de backlinks) pour debug
        homepage_url = max(self.backlinks.items(), key=lambda x: x[1])[0] if self.backlinks else None
        if homepage_url:
            logger.info(f"\nüìç Page suivie (+ de backlinks): {homepage_url}")
            logger.info(f"   Backlinks: {self.backlinks.get(homepage_url, 0)}")
            logger.info(f"   Liens sortants: {len(self.internal_links.get(homepage_url, []))}")
            logger.info(f"   Liens entrants: {self.url_data.get(homepage_url, {}).get('internal_links_received', 0)}")

        # Param√®tres de convergence
        max_iterations = 20
        tolerance = 0.01

        for iteration in range(max_iterations):
            logger.info(f"\nIteration {iteration + 1}/{max_iterations}")

            # Initialiser les nouveaux scores √† Z√âRO (pas une copie!)
            new_scores = {url: 0.0 for url in self.url_scores}

            # √âtape 1: Injecter le jus des backlinks (√† CHAQUE it√©ration)
            for url, count in self.backlinks.items():
                if url in new_scores:
                    backlink_juice = count * self.backlink_score
                    new_scores[url] += backlink_juice

            # √âtape 2: Distribuer les 85% transmis (15% perdus dans le vide)
            for source_url, links in self.internal_links.items():
                if source_url not in self.url_scores:
                    continue

                current_score = self.url_scores[source_url]

                # Jus √† transmettre (85% du score actuel, 15% sont PERDUS)
                juice_to_transmit = current_score * self.transmission_rate

                if juice_to_transmit <= 0:
                    continue

                # Filtrer uniquement les liens vers le domaine principal ET exclure .pdf et param√®tres GET
                links = [l for l in links
                        if urlparse(l['destination']).netloc == self.main_domain
                        and not self._should_exclude_url(l['destination'])]

                # S√©parer les liens par position (Contenu vs Navigation)
                content_links = [l for l in links if l['link_position'] in ['Contenu', 'Content']]
                navigation_links = [l for l in links if l['link_position'] not in ['Contenu', 'Content']]

                # Nombre de liens de chaque type
                num_content = len(content_links)
                num_navigation = len(navigation_links)

                # Calculer le poids total (contenu = 9x, navigation = 1x)
                total_weight = (num_content * self.content_link_weight) + (num_navigation * self.navigation_link_weight)

                if total_weight > 0:
                    # Jus par unit√© de poids
                    juice_per_weight_unit = juice_to_transmit / total_weight

                    # Distribuer aux liens de contenu (poids 9)
                    if num_content > 0:
                        juice_per_content_link = juice_per_weight_unit * self.content_link_weight
                        for link in content_links:
                            dest_url = link['destination']
                            if dest_url in new_scores:
                                new_scores[dest_url] += juice_per_content_link

                    # Distribuer aux liens de navigation (poids 1)
                    if num_navigation > 0:
                        juice_per_nav_link = juice_per_weight_unit * self.navigation_link_weight
                        for link in navigation_links:
                            dest_url = link['destination']
                            if dest_url in new_scores:
                                new_scores[dest_url] += juice_per_nav_link
                # Si total_weight == 0 (pas de liens sortants), les 85% sont perdus dans le vide

            # V√©rifier la convergence
            max_change = max(abs(new_scores[url] - self.url_scores[url]) for url in self.url_scores)

            # Calculer le jus total (√† l'√©quilibre)
            current_total_juice = sum(new_scores.values())

            # Afficher les stats
            max_score = max(new_scores.values()) if new_scores else 0
            logger.info(f"  Jus total: {current_total_juice:.2f}")
            logger.info(f"  Max score: {max_score:.2f} | Max changement: {max_change:.4f}")

            # Debug de la homepage
            if homepage_url and homepage_url in new_scores:
                old_score = self.url_scores.get(homepage_url, 0)
                new_score = new_scores.get(homepage_url, 0)
                logger.info(f"  üìç Homepage: {old_score:.2f} ‚Üí {new_score:.2f} (Œî {new_score - old_score:+.2f})")

            # Mettre √† jour les scores
            self.url_scores = new_scores

            # V√©rifier si convergence atteinte
            if max_change < tolerance:
                logger.info(f"‚úì Convergence atteinte √† l'it√©ration {iteration + 1} (changement < {tolerance})")
                break
        else:
            logger.info(f"‚úì Nombre maximum d'it√©rations atteint ({max_iterations})")

    def _normalize_scores(self):
        """Normalise les scores sur 100"""
        logger.info(f"\n4. NORMALISATION DES SCORES (max = {self.normalize_max})")
        logger.info("-" * 60)

        max_score = max(self.url_scores.values()) if self.url_scores else 1

        # Identifier la page avec le score max
        max_url = max(self.url_scores.items(), key=lambda x: x[1])[0] if self.url_scores else None

        if max_url:
            logger.info(f"\nüìä Page avec score maximum:")
            logger.info(f"   URL: {max_url}")
            logger.info(f"   Score brut: {max_score:.2f}")
            logger.info(f"   Backlinks: {self.url_data.get(max_url, {}).get('backlinks_count', 0)}")
            logger.info(f"   Liens sortants: {self.url_data.get(max_url, {}).get('internal_links_sent', 0)}")
            logger.info(f"   Liens entrants: {self.url_data.get(max_url, {}).get('internal_links_received', 0)}")

        if max_score > 0:
            for url in self.url_scores:
                self.url_scores[url] = (self.url_scores[url] / max_score) * self.normalize_max

        logger.info(f"\nScore maximum avant normalisation: {max_score:.2f}")
        logger.info(f"Score maximum apres normalisation: {self.normalize_max}")

    def _calculate_statistics(self, sf_parser, ahrefs_parser) -> Dict:
        """Calcule toutes les statistiques pour les r√©sultats"""
        logger.info("\n5. CALCUL DES STATISTIQUES")
        logger.info("-" * 60)

        results = {
            'urls': [],
            'total_urls': len(self.url_scores),
            'total_internal_links': len(sf_parser.df),
            'total_backlinks': len(ahrefs_parser.df),
            'has_gsc_data': bool(self.gsc_data),
            'gsc_urls_count': len(self.gsc_data) if self.gsc_data else 0,
            'config': {
                'backlink_score': self.backlink_score,
                'transmission_rate': self.transmission_rate,
                'content_link_weight': self.content_link_weight,
                'navigation_link_weight': self.navigation_link_weight,
                'iterations': self.iterations
            }
        }

        # Pour chaque URL, cr√©er un dictionnaire complet
        for url, score in self.url_scores.items():
            url_info = self.url_data[url]

            # Top 3 des ancres
            anchors = url_info['anchors']
            anchor_counts = Counter(anchors)
            top_3_anchors = anchor_counts.most_common(3)

            url_result = {
                'url': url,
                'seo_score': round(score, 2),
                'backlinks_count': url_info['backlinks_count'],
                'internal_links_received': url_info['internal_links_received'],
                'internal_links_received_content': url_info['internal_links_received_content'],
                'internal_links_received_navigation': url_info['internal_links_received_navigation'],
                'internal_links_sent': url_info['internal_links_sent'],
                'status_code': url_info['status_code'],
                'is_error': url_info['is_error'],
                'top_3_anchors': [{'anchor': anchor, 'count': count} for anchor, count in top_3_anchors],
                'category': self._get_url_category(url)
            }

            # Ajouter les donn√©es GSC si disponibles
            if url in self.gsc_data:
                gsc_info = self.gsc_data[url]
                url_result['gsc_clicks'] = gsc_info.get('total_clicks', 0)
                url_result['gsc_impressions'] = gsc_info.get('total_impressions', 0)
                url_result['gsc_queries_count'] = gsc_info.get('queries_count', 0)
                # Stocker TOUS les mots-cl√©s avec leurs donn√©es individuelles
                url_result['gsc_keywords'] = gsc_info.get('keywords', [])
                # Filtrer les mots-cl√©s avec >= 50 impressions pour le meilleur mot-cl√©
                keywords_with_impressions = [
                    kw for kw in url_result['gsc_keywords']
                    if kw.get('impressions', 0) >= 50
                ]
                # Calculer le meilleur mot-cl√© (position la plus basse) parmi ceux avec >= 50 impressions
                if keywords_with_impressions:
                    url_result['gsc_best_keyword'] = min(
                        keywords_with_impressions,
                        key=lambda x: x['position']
                    )
                else:
                    url_result['gsc_best_keyword'] = None
            else:
                url_result['gsc_clicks'] = None
                url_result['gsc_impressions'] = None
                url_result['gsc_queries_count'] = None
                url_result['gsc_keywords'] = []
                url_result['gsc_best_keyword'] = None

            results['urls'].append(url_result)

        # Trier par score d√©croissant
        results['urls'].sort(key=lambda x: x['seo_score'], reverse=True)

        # Calculer la m√©diane des scores SEO
        scores = sorted([u['seo_score'] for u in results['urls']])
        n = len(scores)
        if n > 0:
            if n % 2 == 0:
                results['median_seo_score'] = (scores[n//2 - 1] + scores[n//2]) / 2
            else:
                results['median_seo_score'] = scores[n//2]
        else:
            results['median_seo_score'] = 0

        # Stats par cat√©gorie
        results['categories'] = self._calculate_category_stats(results['urls'])

        # Pages sources de jus (celles avec le plus de backlinks)
        results['top_juice_sources'] = sorted(
            [u for u in results['urls'] if u['backlinks_count'] > 0],
            key=lambda x: x['backlinks_count'],
            reverse=True
        )[:10]

        # Pages en erreur recevant des liens
        results['error_pages_with_links'] = [
            u for u in results['urls']
            if u['is_error'] and u['internal_links_received'] > 0
        ]

        # Calcul du taux de jus envoy√© sur des erreurs
        total_links_to_errors = sum(u['internal_links_received'] for u in results['error_pages_with_links'])
        results['error_juice_rate'] = (total_links_to_errors / results['total_internal_links'] * 100) if results['total_internal_links'] > 0 else 0

        # Distribution du jus SEO par code de statut
        results['juice_by_status'] = self._calculate_juice_by_status(results['urls'])

        # G√©n√©rer les recommandations automatiques
        results['recommendations'] = self._generate_recommendations(results)

        logger.info(f"URLs analysees: {results['total_urls']}")
        logger.info(f"Pages sources de jus: {len(results['top_juice_sources'])}")
        logger.info(f"Pages en erreur recevant des liens: {len(results['error_pages_with_links'])}")
        logger.info(f"Taux de jus sur erreurs: {results['error_juice_rate']:.2f}%")

        return results

    def _calculate_juice_by_status(self, urls: List[Dict]) -> Dict:
        """Calcule la distribution du jus SEO par code de statut"""
        status_groups = {
            '200': 0,
            '3xx': 0,
            '4xx': 0,
            '5xx': 0,
            'Autre': 0
        }

        for url_data in urls:
            status_code = url_data['status_code']
            juice = url_data['seo_score']

            if status_code == 200:
                status_groups['200'] += juice
            elif 300 <= status_code < 400:
                status_groups['3xx'] += juice
            elif 400 <= status_code < 500:
                status_groups['4xx'] += juice
            elif 500 <= status_code < 600:
                status_groups['5xx'] += juice
            else:
                status_groups['Autre'] += juice

        # Arrondir les valeurs
        for key in status_groups:
            status_groups[key] = round(status_groups[key], 2)

        return status_groups

    def _get_url_category(self, url: str) -> str:
        """D√©termine la cat√©gorie d'une URL bas√©e sur son chemin"""
        try:
            from urllib.parse import urlparse
            path = urlparse(url).path

            if not path or path == '/':
                return 'Homepage'

            # Extraire le premier segment du chemin
            segments = [s for s in path.split('/') if s]
            if segments:
                return segments[0].capitalize()

            return 'Autre'
        except:
            return 'Autre'

    def _calculate_category_stats(self, urls: List[Dict]) -> Dict:
        """Calcule les statistiques par cat√©gorie"""
        categories = defaultdict(lambda: {'count': 0, 'total_score': 0, 'avg_score': 0})

        for url_data in urls:
            category = url_data['category']
            categories[category]['count'] += 1
            categories[category]['total_score'] += url_data['seo_score']

        # Calculer les moyennes
        for category in categories:
            categories[category]['avg_score'] = round(
                categories[category]['total_score'] / categories[category]['count'], 2
            )

        return dict(categories)

    def _generate_recommendations(self, results: Dict) -> List[Dict]:
        """
        G√©n√®re des recommandations automatiques bas√©es sur l'analyse

        Returns:
            Liste de recommandations avec priorit√©, type, titre, description et actions
        """
        recommendations = []

        # 1. Recommandation sur les pages en erreur recevant des liens
        error_pages = results.get('error_pages_with_links', [])
        if error_pages:
            total_error_links = sum(p['internal_links_received'] for p in error_pages)
            recommendations.append({
                'id': 'error_pages',
                'priority': 'critical',
                'type': 'technical',
                'icon': 'exclamation-triangle-fill',
                'title': f"Corriger {len(error_pages)} pages en erreur",
                'description': f"Ces pages re√ßoivent {total_error_links} liens internes mais retournent des codes d'erreur. Le jus SEO envoy√© vers ces pages est perdu.",
                'impact': f"{results.get('error_juice_rate', 0):.1f}% du jus SEO est gaspill√© sur des erreurs",
                'actions': [
                    {'text': f"Rediriger ou corriger la page {p['url'][:50]}... (re√ßoit {p['internal_links_received']} liens)", 'url': p['url']}
                    for p in sorted(error_pages, key=lambda x: x['internal_links_received'], reverse=True)[:5]
                ]
            })

        # 2. Recommandation Quick Wins (positions 5-12)
        if results.get('has_gsc_data'):
            quick_wins = []
            for url_data in results['urls']:
                for kw in url_data.get('gsc_keywords', []):
                    if 5 <= kw.get('position', 100) <= 12 and kw.get('impressions', 0) >= 50:
                        quick_wins.append({
                            'url': url_data['url'],
                            'keyword': kw['query'],
                            'position': kw['position'],
                            'impressions': kw['impressions'],
                            'seo_score': url_data['seo_score']
                        })

            if quick_wins:
                # Trier par impressions d√©croissantes
                quick_wins.sort(key=lambda x: x['impressions'], reverse=True)
                recommendations.append({
                    'id': 'quick_wins',
                    'priority': 'high',
                    'type': 'opportunity',
                    'icon': 'trophy-fill',
                    'title': f"{len(quick_wins)} Quick Wins identifi√©s",
                    'description': "Ces mots-cl√©s sont en position 5-12. Un renforcement du maillage interne vers ces pages pourrait les faire passer en top 3.",
                    'impact': f"Potentiel de {sum(qw['impressions'] for qw in quick_wins[:10]):,} impressions suppl√©mentaires en page 1",
                    'actions': [
                        {'text': f"'{qw['keyword'][:40]}' - Position {qw['position']:.0f} ({qw['impressions']:,} imp.)", 'url': qw['url']}
                        for qw in quick_wins[:5]
                    ]
                })

        # 3. Recommandation sur les pages qui gaspillent le jus
        if results.get('has_gsc_data'):
            median_score = results.get('median_seo_score', 0)
            wasteful_pages = []
            for url_data in results['urls']:
                if url_data['seo_score'] > median_score:
                    best_kw = url_data.get('gsc_best_keyword')
                    if not best_kw or best_kw.get('position', 100) > 12:
                        wasteful_pages.append(url_data)

            if wasteful_pages:
                total_wasted_juice = sum(p['seo_score'] for p in wasteful_pages)
                recommendations.append({
                    'id': 'wasteful_pages',
                    'priority': 'medium',
                    'type': 'optimization',
                    'icon': 'graph-down-arrow',
                    'title': f"{len(wasteful_pages)} pages gaspillent du jus SEO",
                    'description': f"Ces pages ont un score SEO sup√©rieur √† la m√©diane ({median_score:.1f}) mais ne g√©n√®rent pas de trafic organique (position > 12 ou pas de mot-cl√©).",
                    'impact': f"{total_wasted_juice:.0f} points de jus SEO mal utilis√©s",
                    'actions': [
                        {'text': f"{p['url'][:50]}... (Score: {p['seo_score']:.1f})", 'url': p['url']}
                        for p in sorted(wasteful_pages, key=lambda x: x['seo_score'], reverse=True)[:5]
                    ]
                })

        # 4. Recommandation sur les pages avec beaucoup de backlinks mais peu de liens internes sortants
        pages_hoarding_juice = [
            u for u in results['urls']
            if u['backlinks_count'] > 0 and u['internal_links_sent'] < 5
        ]
        if pages_hoarding_juice:
            pages_hoarding_juice.sort(key=lambda x: x['backlinks_count'], reverse=True)
            recommendations.append({
                'id': 'juice_hoarding',
                'priority': 'medium',
                'type': 'optimization',
                'icon': 'box-arrow-in-right',
                'title': f"{len(pages_hoarding_juice)} pages retiennent leur jus SEO",
                'description': "Ces pages re√ßoivent des backlinks externes mais ne distribuent pas suffisamment leur jus SEO via des liens internes (moins de 5 liens sortants).",
                'impact': "Am√©liorer la distribution du jus pour renforcer les pages strat√©giques",
                'actions': [
                    {'text': f"{p['url'][:50]}... ({p['backlinks_count']} backlinks, {p['internal_links_sent']} liens sortants)", 'url': p['url']}
                    for p in pages_hoarding_juice[:5]
                ]
            })

        # 5. Recommandation sur les pages isol√©es (peu de liens entrants)
        avg_links_received = sum(u['internal_links_received_content'] for u in results['urls']) / len(results['urls']) if results['urls'] else 0
        isolated_pages = [
            u for u in results['urls']
            if u['internal_links_received_content'] < 2 and u['seo_score'] < median_score
        ]
        if isolated_pages and len(isolated_pages) > 5:
            recommendations.append({
                'id': 'isolated_pages',
                'priority': 'low',
                'type': 'structure',
                'icon': 'diagram-2',
                'title': f"{len(isolated_pages)} pages re√ßoivent peu de liens de contenu",
                'description': f"Ces pages re√ßoivent moins de 2 liens depuis le contenu (moyenne du site: {avg_links_received:.1f}). Elles sont difficiles √† d√©couvrir pour les moteurs de recherche.",
                'impact': "Am√©liorer l'indexation et la distribution du jus SEO",
                'actions': [
                    {'text': f"{p['url'][:50]}... ({p['internal_links_received_content']} liens contenu)", 'url': p['url']}
                    for p in sorted(isolated_pages, key=lambda x: x['internal_links_received_content'])[:5]
                ]
            })

        # Trier par priorit√©
        priority_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}
        recommendations.sort(key=lambda x: priority_order.get(x['priority'], 99))

        logger.info(f"Recommandations generees: {len(recommendations)}")

        return recommendations
